# Alternating Least Squares (ALS)
$$\text{als loss function:}\sum(r_{i,j}-x^T_u*y_i) + \lambda(||x_u||^2 + ||y_i||^2)$$
- $r_{}$ - csr_matrix
- $x$ - матрица пользователей
- $y$ - матрица айтемов
- $\lambda$ - коэффициент регуляризации
- $||x_u||^2 = \sum x_i^2$ - норма вектора $x_u$
- $||y_i||^2 = \sum y_i^2$ - норма вектора $y_i$

### **Итерационный порядок работы ALS**:
1. **Инициализация**: 
   - Матрицы пользователей $X$ (размер $n \times k$) и айтемов $Y$ (размер $m \times k$) инициализируются случайно (или через SVD для ускорения сходимости). Здесь $k$ — размерность латентных факторов.

2. **Фиксация одной матрицы**:
   - На каждом шаге фиксируется одна из матриц. Например, сначала фиксируется $Y$, и оптимизируется $X$, затем фиксируется $X$, и оптимизируется $Y$. Это чередование (alternating) и даёт название алгоритму.

3. **Оптимизация с регуляризацией**:
   - **Функция потерь** для пользователей:  
     $$
     \min_X \sum_{(u,i) \in \text{observed}} \left( r_{u,i} - x_u^T y_i \right)^2 + \lambda \left( \|x_u\|^2 + \|y_i\|^2 \right),
     $$
     где сумма берётся только по наблюдениям (ненулевым элементам $R$).
   - При фиксированной $Y$ задача становится **выпуклой** по $X$, и её можно решить **аналитически** через нормальное уравнение:
     $$
     x_u = \left( Y^T Y + \lambda I \right)^{-1} Y^T r_u,
     $$
     где $r_u$ — вектор оценок пользователя $u$.

4. **Переключение фиксации**:
   - После обновления $X$ фиксируется $X$, и аналогично обновляется $Y$:
     $$
     y_i = \left( X^T X + \lambda I \right)^{-1} X^T r_i,
     $$
     где $r_i$ — вектор оценок айтема $i$.

5. **Итерации до сходимости**:
   - Шаги 2–4 повторяются, пока изменения в функции потерь не станут меньше порога или не будет достигнуто максимальное число итераций.


```
Инициализировать X и Y случайно
для каждой итерации:
    # Шаг 1: Фиксируем Y, обновляем X
    для каждого пользователя u:
        собрать Y_u (айтемы, с которыми взаимодействовал u)
        вычислить x_u = (Y_u^T Y_u + λI)^−1 Y_u^T r_u
    # Шаг 2: Фиксируем X, обновляем Y
    для каждого айтема i:
        собрать X_i (пользователи, взаимодействовавшие с i)
        вычислить y_i = (X_i^T X_i + λI)^−1 X_i^T r_i
```

> **Пакетное обновление**: В рамках одной итерации ALS обновляются **все пользователи**, затем **все айтемы**.

---

# Пример
---

### **Шаг 1: Исходные данные**
Пусть у нас есть матрица взаимодействий $R$ размером $3 \times 3$ (3 пользователя, 3 айтема), где известны только некоторые оценки:

$$
R = \begin{bmatrix}
5 & 0 & 2 \\
0 & 3 & 0 \\
4 & 0 & 0 \\
\end{bmatrix}
$$

Здесь:
- `0` означает отсутствие взаимодействия (пропущенное значение).
- Например, пользователь 1 поставил айтему 1 оценку 5, а айтему 3 — оценку 2.

---

### **Шаг 2: Инициализация матриц**
Выберем размерность латентных факторов $k = 2$. Инициализируем матрицы пользователей $X$ и айтемов $Y$ случайными значениями:

$$
X = \begin{bmatrix}
0.5 & 0.1 \\
0.3 & 0.2 \\
0.4 & 0.6 \\
\end{bmatrix}, \quad
Y = \begin{bmatrix}
0.2 & 0.7 \\
0.5 & 0.3 \\
0.1 & 0.4 \\
\end{bmatrix}
$$

---

### **Шаг 3: Фиксация Y и обновление X**
#### **Пользователь 1 (u=0)**:
- Известные оценки: $r_{0,0} = 5$, $r_{0,2} = 2$.
- Соответствующие строки матрицы $Y$: $y_0 = [0.2, 0.7]$, $y_2 = [0.1, 0.4]$.

**Собираем подматрицу $Y_u$ для пользователя 1**:
$$
Y_u = \begin{bmatrix}
0.2 & 0.7 \\
0.1 & 0.4 \\
\end{bmatrix}, \quad r_u = \begin{bmatrix}5 \\ 2\end{bmatrix}
$$

**Вычисляем $x_u$**:
1. Рассчитаем $Y_u^T Y_u + \lambda I$ (пусть $\lambda = 0.1$):
$$
Y_u^T Y_u = \begin{bmatrix}
0.2^2 + 0.1^2 & 0.2 \cdot 0.7 + 0.1 \cdot 0.4 \\
0.7 \cdot 0.2 + 0.4 \cdot 0.1 & 0.7^2 + 0.4^2 \\
\end{bmatrix} = \begin{bmatrix}
0.05 & 0.18 \\
0.18 & 0.65 \\
\end{bmatrix}
$$
$$
Y_u^T Y_u + \lambda I = \begin{bmatrix}
0.05 + 0.1 & 0.18 \\
0.18 & 0.65 + 0.1 \\
\end{bmatrix} = \begin{bmatrix}
0.15 & 0.18 \\
0.18 & 0.75 \\
\end{bmatrix}
$$

2. Обратная матрица:
$$
\left(Y_u^T Y_u + \lambda I\right)^{-1} \approx \begin{bmatrix}
9.36 & -2.25 \\
-2.25 & 1.87 \\
\end{bmatrix}
$$

1. Умножаем на $Y_u^T r_u$:
$$
Y_u^T r_u = \begin{bmatrix}
0.2 \cdot 5 + 0.1 \cdot 2 \\
0.7 \cdot 5 + 0.4 \cdot 2 \\
\end{bmatrix} = \begin{bmatrix}1.2 \\ 4.3\end{bmatrix}
$$
$$
x_u = \begin{bmatrix}
9.36 & -2.25 \\
-2.25 & 1.87 \\
\end{bmatrix} \cdot \begin{bmatrix}1.2 \\ 4.3\end{bmatrix} \approx \begin{bmatrix}1.57 \\ 5.35\end{bmatrix}
$$

**Обновлённая матрица $X$**:
$$
X = \begin{bmatrix}
1.57 & 5.35 \\
0.3 & 0.2 \\
0.4 & 0.6 \\
\end{bmatrix}
$$

---

### **Шаг 4: Фиксация X и обновление Y**
Теперь фиксируем обновлённую $X$ и повторяем шаг для айтемов.

#### **Айтем 1 (i=0)**:
- Известные оценки: $r_{0,0} = 5$, $r_{2,0} = 4$.
- Соответствующие строки $X$: $x_0 = [1.57, 5.35]$, $x_2 = [0.4, 0.6]$.

**Собираем подматрицу $X_i$**:
$$
X_i = \begin{bmatrix}
1.57 & 5.35 \\
0.4 & 0.6 \\
\end{bmatrix}, \quad r_i = \begin{bmatrix}5 \\ 4\end{bmatrix}
$$

**Вычисляем $y_i$** аналогично шагу 3. После расчётов получим обновлённые значения для $Y$.

---

### **Шаг 5: Повторение итераций**
- Чередуем обновление $X$ и $Y$ до сходимости (например, 10 итераций).
- На каждой итерации пересчитываем векторы для всех пользователей и айтемов.

---

### **Итоговые предсказания**
После нескольких итераций матрицы $X$ и $Y$ приблизятся к таким значениям, что их произведение $X \cdot Y^T$ даст предсказанные оценки. Например:

$$
X \cdot Y^T \approx \begin{bmatrix}
5.0 & 3.1 & 2.0 \\
2.3 & 3.0 & 1.5 \\
4.0 & 2.4 & 1.8 \\
\end{bmatrix}
$$

Здесь:
- Предсказанная оценка пользователя 1 для айтема 2 — 3.1 (в исходных данных была 0).
- Предсказанная оценка пользователя 3 для айтема 3 — 1.8.

---

### **Замечания**:
1. **Разреженность**: Алгоритм работает только с ненулевыми элементами матрицы $R$, что экономит вычисления.
2. **Регуляризация**: $\lambda$ (например, 0.1) помогает избежать переобучения.
3. **Сходимость**: Обычно хватает 10–20 итераций для стабилизации функции потерь.

Этот пример иллюстрирует, как ALS "учит" латентные факторы, чтобы восстановить исходные взаимодействия и предсказать пропущенные значения.
**ALS (Alternating Least Squares):** ALS является методом коллаборативной фильтрации, использующим матричное разложение для выделения скрытых факторов взаимодействия между пользователями и продуктами.
Алгоритм Alternating Least Squares (ALS) является популярным методом для решения задач матричной факторизации в рекомендательных системах. Основная цель ALS — факторизовать пользователь-товарную матрицу предпочтений на две матрицы более низкого ранга: одну для пользователей и одну для товаров. Давайте разберемся, как это работает подробно.

### Основные понятия

1. **Матричная факторизация**:
   - Пользователь-товарная матрица $R$ размера $m \times n$, где $m$ — количество пользователей, а $n$ — количество товаров.
   - Разложение $R \approx U \cdot V^T$, где $U$ — матрица пользователей размера $m \times k$, а $V$ — матрица товаров размера $n \times k$. Здесь $k$ — размерность скрытого пространства (обычно значительно меньше $m$ и $n$).

2. **Формализация задачи**:
   - Цель состоит в минимизации функции потерь:
     $$
     \min_{U, V} \sum_{(i, j) \in \mathcal{R}} (R_{ij} - U_i^T V_j)^2 + \lambda (||U||^2 + ||V||^2)
     $$
     где $\mathcal{R}$ — множество известных рейтингов, а $\lambda$ — параметр регуляризации для предотвращения переобучения.

### Шаги алгоритма ALS

1. **Инициализация**:
   - Инициализируем матрицы $U$ и $V$ случайными значениями или с помощью других методов.

2. **Черезмерное упрощение задачи**:
   - ALS разлагает проблему в двухступенчатую процедуру, чередуя оптимизацию по $U$ при фиксированном $V$ и по $V$ при фиксированном $U$.

3. **Оптимизация $U$ при фиксированном $V$**:
   - Когда матрица товаров $V$ зафиксирована, проблема становится линейной регрессией для каждого пользователя $i$:
     $$
     \min_{U_i} \sum_{j \in \mathcal{R}_i} (R_{ij} - U_i^T V_j)^2 + \lambda ||U_i||^2
     $$
   - Решение этой задачи можно найти путем решения системы линейных уравнений:
     $$
     U_i = (V_{\mathcal{R}_i}^T V_{\mathcal{R}_i} + \lambda I)^{-1} V_{\mathcal{R}_i}^T R_{\mathcal{R}_i}
     $$
     где $V_{\mathcal{R}_i}$ — матрица строк $V$, соответствующих известным рейтингам пользователя $i$, а $R_{\mathcal{R}_i}$ — вектор этих рейтингов.

4. **Оптимизация $V$ при фиксированном $U$**:
   - Аналогично, при фиксированной матрице пользователей $U$, решаем задачу для каждого товара $j$:
     $$
     \min_{V_j} \sum_{i \in \mathcal{R}_j} (R_{ij} - U_i^T V_j)^2 + \lambda ||V_j||^2
     $$
   - Решение:
     $$
     V_j = (U_{\mathcal{R}_j}^T U_{\mathcal{R}_j} + \lambda I)^{-1} U_{\mathcal{R}_j}^T R_{\mathcal{R}_j}
     $$
     где $U_{\mathcal{R}_j}$ — матрица строк $U$, соответствующих известным рейтингам товара $j$, а $R_{\mathcal{R}_j}$ — вектор этих рейтингов.

5. **Чередование**:
   - Повторяем шаги оптимизации $U$ и $V$ до тех пор, пока функция потерь не стабилизируется или не достигнет заданного количества итераций.


### Пример:
Допустим, у вас есть матрица взаимодействий для рекомендательной системы фильмов:

| Пользователь/Фильм | Фильм 1 | Фильм 2 | Фильм 3 | Фильм 4 |
|--------------------|---------|---------|---------|---------|
| Пользователь 1     | 5       | 3       | ?       | 1       |
| Пользователь 2     | 4       | ?       | 2       | 1       |
| Пользователь 3     | ?       | 4       | 5       | 3       |
| Пользователь 4     | 1       | 1       | ?       | 5       |

ALS разложит эту матрицу на две матрицы:

- Матрица пользователей \( U \): где строки содержат предпочтения пользователей по скрытым факторам.
- Матрица фильмов \( V \): где столбцы содержат характеристики фильмов.

В результате перемножения этих матриц вы получите приблизительную версию исходной матрицы, но с заполненными пропусками. Например, вы можете обнаружить, что Пользователю 1 наверняка понравится Фильм 3, поскольку он высоко оценил другие фильмы с похожими характеристиками.

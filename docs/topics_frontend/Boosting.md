# Деревья

## Классификация

### Энтропия 
Мера неопределенности - чем выше энтропия, тем меньше ясно какой исход из множества событий вероятнее произойдет 

$$H(p_1, ..., p_n) = -\sum^n_{i=1}p_i\log p_i$$
### Критерий Джини 


Этот критерий оценивает вероятность того, что случайно выбранный элемент будет неверно классифицирован, если его классифицировать по распределению классов в группе.
$$H(p_1, ..., p_K) = \sum^K_{i=1}p_i(1-p_i)$$
Энтропия может иногда давать более сбалансированные разбиения, тогда как Джини имеет тенденцию предпочитать разбиения с одним доминирующим классом.


### Критерий информативности 

Каким образом мы будем определять хорошо ли вышло разбиение. 
$$\Large Q(R, j, t) = H(R) - \frac{|R_{l}|}{|R|}H(R_l) - \frac{|R_l|}{|R|}H(R_r)\rightarrow \max_{j,t}$$
$j$ - признак разбиения 
$t$ - пороговое значение 
$H(R)$ - хаотичность родительской вершины 
$H(R_l)$ - хаотичность левой дочерней вершины
$H(R_r)$ - хаотичность правой дочерней вершины
$R_l$ - количество объектов прошедших в левую дочернюю вершину
$R_r$ - количество объектов прошедших в правую дочернюю вершину
$R$ - количество объектов в родительской вершине

## Регрессия 

В случае регресии все решается через дисперсию

$$\Large H(R) = \frac{1}{|R|}{\sum_{x_i, y_i \in R}}(y_i - y_R)^2$$
$$\large y_R = \frac{1}{|R|}\sum_{(x_i, y_i)\in R}y_i$$


для неявного таргета типа клики

Задачи:
- [[3.2. Библиотека implicit#Item2Item kNN|Item2Item kNN]]
- Logistic matrix factorization
- [[3.2. Библиотека implicit#implicit iALS|implicit ALS]]
- Bayesian Personalized Ranking

----

## Item2Item kNN
пользователь - вектор-строка 
объект - вектор-столбец

на их основе оцениваем схожесть и строим рекомендации:
- Косинусное расстрояние 
- Корреляция Пирсона

Модели:
- CosineRecommender
- BM25Recommender
- TFIDFRecommender

Гиперпараметры:
- k - число соседей (мин 30)

Обучение:
- Поиск k ближайших соседей для каждого item и сохранение схожести с ними

Построение рекомендаций:
1. Находит топ-k соседей для каждого объекта с которым было взаимодействие 
2. Берем и выдаем все топ-k (set-аем)

```python
cosine_model = CosineRecommender(K=10)
cosine_model.fit(train_df.T) # строки - объекты, столбцы - пользователи

recs = cosine_model.recommend(row_id,
							 train_df,
							 N=top_N, 
							 filter_already_liked_items=True,
							 )
recs = pd.DataFrame(recs, columns=['cols_id', 'similarity'])

similar_items('item_name', model) # возвращает список похожих
```

---

## implicit iALS
__Предсказывает было ли взаимодействие__


implicit - переход от предсказания значений к предсказанию релевантности, учитывая веса 
ALS - метод оптимизации, в котором мы поочередно фиксируем одну матрицу и делаем оптимизационный шаг по другой
![[Pasted image 20240526234840.png]]
Ну тип берем чела ($p_u$)и считаем по нему вектор объектов $q_i$ и так итеративно от объекта к пользователю и наоборот 

Функция ошибки: $$L = \sum_{u,i}c_{ui}(r_{ui}-p_uq^T_i)^2+\lambda(\sum_u||p_u||^2 + \sum_i||q_i||^2)$$
- $c_{ui}$ - степень уверенности (не меньше 1)
- $r_{ui}$ - факт взаимодействия (0 или 1)


Выходы модели не интерпритируются и используются только для реранжирования


---

## LightFM
Мы обучаем вектора не для самих пользователей а для каждой конкретной фичи и пользователей или объектов представляем как сумму векторов его фичей

На базе матрицы взаимодействий $(R \in \mathbb{R}^{N\times M})$ создаются две другие матрицы:
- User feature matrix $(F \times N)$
- Item feature matrix $(F \times M)$


losses:
- Logistic
- Bayesian Personalized Ranking
- Weighted Approximate-Rank Pairwise (WARP)
- k-os WARP

Оптимизаторы:
- adagrad
- adadelta


### Как собираем датасет
lightfm.data.Dataset
- users - ids пользователей
- items - ids объектов
- user_features - имена фичей пользователей
- item_features - имена фичей объектов

### Методы 
- build_interactions - построение матицы взаимодействий ($R$)
	- (user_id, item_id)
	- (user_id, item_id, weight)
- build_user_features/build_item_features - построение тех самых двух матриц фичей пользователей/объектов 
	- (user_id, \[user_feature_name1, user_feature_name2\])
	- (user_id, {user_feature_name1: weight})

### LightFM WARP

Переход от проверки качества предсказанного значения ($r_{ui}$) к предсказанию качества ранжирования

```python
lfm_model = LightFM(no_components=64,
				   learning_rate=0.05,
				   loss='warp',
				   max_sampled=5,
				   random_state=1234)
num_epochs = 15

for _ in tqdm(range(num_epochs), total=num_epochs):
	lfm_model.fit_partial( # дообучение модели на имеющихся данных
		train_df,
		user_features=train_user_features,
		item_features=train_item_features,
		num_threads=4)
```

model_predict('user_id', item_ids )
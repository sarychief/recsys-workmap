### Модель: _DSSM - Deep Structured Semantic Model (2013)_
> _Первая моделька для использования контентных фичей. Берет u и i в последовательных слоях сжимает до одной размерности и полученные эмбеддинги сравниваем косинусной схожестью а потом отдает в softmax для получения вероятностей_


![](https://recbole.io/docs/_images/dssm.png)
#### Полное описание  
В концепции DSSM у нас имеется запрос $Q$ и документы $\{D_i\}^N_1$, которые можно свести запрос - это пользователь $u$, а документ - это айтемы $i$.

#### Работа алгоритма 
__Шаги:__  
1. Term Vector: 
	- $Q$ разбивается на триграммы.
2. Word Hashing: 
	- Разреженные векторы триграмм преобразуются в плотные векторные представления.
	- Каждая триграмма кодируется как **бинарный вектор** (one-hot) размерности ~30k (все возможные уникальные триграммы в корпусе).
3. Multi-layer non-linear projection: 
	- Разреженный вектор триграмм (30k измерений) умножается на **матрицу весов** $W\in \mathbb{R}^{d\times30k}$, где $d$ — размерность эмбеддинга (например, 300): $$h=W⋅x_{tri}$$
	- Несколько полносвязных слоев сжимают вектор до низкоразмерного семантического пространства (например, 128 измерений), используя нелинейные функции активации (например, `tanh`), чтобы уловить сложные паттерны. 
4. Semantic feature: 
	- На выходе последнего слоя получаются **семантические векторы** запроса и документа, которые кодируют их смысл.
5. Relevance measured by cosine similarity: 
	- Схожесть между вектором запроса и документа вычисляется через косинусное расстояние:  $$\text{cos}(q, d) = \frac{\mathbf{q} \cdot \mathbf{d}}{\|\mathbf{q}\| \cdot \|\mathbf{d}\|}.$$
6. Posterior probability computed by softmax: 
	- Косинусная схожесть преобразуется в вероятность с помощью softmax:  $$P(d_i | q) = \frac{\exp(\gamma \cdot \text{cos}(q, d_i))}{\sum_{d_j \in D} \exp(\gamma \cdot \text{cos}(q, d_j))},$$

7. Negative Log-Likelihood (NLL):
	- На вход подаётся позитивная пара (запрос $q$, релевантный документ $d^+$) и несколько негативных пар (запрос $q$, случайные нерелевантные документы $d_i^−$​).$$\mathcal{L}=−\log(P(d^+∣q)).$$
	- это стандартный способ вычисления кросс-энтропийной потери, когда целевой класс имеет вероятность 1, астальные классы 0.

#### Параметры алгоритма
- $\gamma$ — параметр масштабирования (чтобы косинус в районе 0 не колебался). чем больше $\gamma$, тем больше штраф за ошибки в топе.
- 

#### Особенности алгоритма
-  Word hashing - это способ уменьшить размерность через триграммы
- Term Vector - это не вектора пользователя/айтема, а представление текста через триграммы
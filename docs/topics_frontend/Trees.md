- Построение дерева
- Параметры дерева
- Прунинг
---
- **Корень дерева** - вершина, из которой исходят ребра, но не входит ни одно ребро
- **Ребра -** отрезки, исходящие из вершин дерева
- **Предикат -** условие, которое проверяется в вершине дерева
- **Лист** - концевая вершина дерева; в листьях дерева стоят предсказания модели

# Построение дерева
как найти наилучший признак и порог для предиката? 

## Критерий информативности - $H(R)$
- Чем меньше $H(R)$ тем более однородная (одинаковая) выборка
- Чем больше $H(R)$ тем менее однородная (одинаковая) выборка

В задаче бинарной классификации мы хотим, чтобы в избранной вершине прошедшая выборка была максимально однородна (все объекты были одного класса)![](https://ucarecdn.com/0d26a98e-d1d8-4443-9a22-78079323dabd/)
### Задача минимизации

Мы ищем такой признак $x_j$​ и такой порог $t$ для него, что разбиение объектов по предикату $x_j​>t$ минимизирует взвешенную сумму критериев информативности.
$$\frac{|R_l|}{|R|}H(R_l)+\frac{|R_r|}{|R|}H(R_r)\rightarrow\min_{x_j,t}$$
- $|R|$ - размер исходной выборки
- $|R_l|$ и $|R_r|$ - размер левой и правой подвыборки после деления по предикату исходной подвыборки
- $H(R_l)$ и $H(R_r)$ критерии информативности левой и правой подвыборки после деления по предикату исходной подвыборки
- $x_j$ - j-ый признак, который мы пытаемся разделить 
- $t$ - порог деления $j$-го признака 

### Задача максимизации - Information Gain (прирост информации)
$$IG = H(R) - \frac{|R_l|}{|R|}H(R_l) - \frac{|R_r|}{|R|}H(R_r)\rightarrow\max_{x_j,t}$$

### Как искать $H(R)$?

#### Регрессия 
$H(R)$ в задаче регрессии это дисперсия $\frac{1}{n}\sum^n_{i=1}(y_i-\text{mean}(y))^2$

#### Жесткая классификация 
$H(R)$ в задаче жесткой классификации (.predict()) считается как доля объектов не самого популярного класса (ошибка классификации)

#### Мягкая классификация 
Мягкая классификация, это когда на каждый объект выдается вектор вероятностей 

> Вспомни, когда вызываешь метод .predict_proba() тебе на выходе дается матрица размером $m\times n$ где n - количество классов, а $x_{ij}$ это вероятность отношения $i$-го объекта к $j$-му классу

тогда на выходе мягкой классификации для каждого объекта формируется вектор вероятностей отношения к каждому классу. 

Для этой задачи есть два метода расчета $H(R)$:
![](https://ucarecdn.com/7f21d565-d426-4260-bc60-53320b0b6080/)
1. __Энтропия__: $$H(R) = \sum^K_{k=1}p_k\log(p_k)$$
2. __Критерий Джини__: $$H(R) = \sum^K_{k=1}p_k(1-p_k)$$
где $p_k$​ - доля объектов $k$-го класса в вершине, а $K$ - число классов в задаче.


- Если при построении дерева используется энтропийный критерий, то такой алгоритм в литературе называется **ID3 (Iterative Dichotomiser 3)**
- Если же используется критерий Джини, то алгоритм называется **CART (Classification And Regression Tree)**.



----
# Параметры дерева
- _criterion_ - критерий информативности, используемый при построении дерева
- _max_depth_ - максимальная глубина дерева
- _min_samples_split_ - минимальное количество объектов, которые должны находиться в вершине, чтобы её дальше разбивать
- _min_samples_leaf_ - минимальное количество объектов, которое находится в листе (если после разбиения в одной из полученных подгрупп число объектов меньше, чем min_samples_leaf, то разбиение не производится)
- _max_features_ - число признаков, используемых для поиска наилучшего предиката в каждой вершине.

---
# Pruning 
Это способ упрощения дерева
## _Cost-Complexity pruning_ - один из методов
крч просто регуляризация числа вершин дерева 

$Q\alpha(T)=Q(T)+\alpha|T|$

где $|T|$ - число вершин в дереве $T$, $\alpha$ - коэффициент регуляризации.

При оптимизации такого функционала в итоговом дереве штрафуется число вершин, то есть построенное дерево будет содержать меньше вершин, чем без регуляризации - значит, будет проще и менее переобученным.

### Преимущества прунинга
1. Уменьшает сложность модели.
2. Улучшает обобщающую способность.
3. Делает модель более интерпретируемой.
### Недостатки прунинга
1. Требует тщательного подбора параметров (например, `ccp_alpha`).
2. Может привести к потере важной информации, если выполнен слишком агрессивно.

## Кодировщики 

самые популярные енкодеры :
- Для GPT-like models: Byte Pair Encoding (__BPE__)
- Для BERT-like models : WordPrice + SentencePiece

Выбор правильного токеназейра очень важен для мультимодельных моделей и от их сложности. 
Чем ниже словарь токенайзера, тем вероятно хуже он будет разбивать текст на токены, что в свою очередь может привести к неправильных инференсам модели и потери денег. 

---
## Трансформеры

Архитектура трансформер состоит из двух частей, каждая из которых многократно повторяется, прежде чем отдать значения дальше

---

# Encoder block

- Feed Forward Neural Network
	- отвечает за computation 
		- Этот блок отвечает за увеличение параметров модели тем самым увеличивая качество
- Self-Attention
	- отвечает за Communication 
		- Этот блок отвечает за подчеркивание важности связок слов 


#### Feed-Forward слой

3 слоя: $D_m \rightarrow D_{ff} \rightarrow D_m$
Для BERT-Based: D=768, K=3072 (ff)
1. **Первое линейное преобразование**: Входные данные (размерности d) преобразуются с помощью матрицы весов, получая промежуточный слой размерности d_ff.
2. **Применение функции активации**: К промежуточному слою применяется функция активации (обычно ReLU).
3. **Второе линейное преобразование**: Промежуточный слой преобразуется обратно в размерность d.

#### Self-attention
Идея в подсчете для каждого слоя связи со всеми словами. 

Алгоритм: 
1) для каждого эмбеддинга слова считается три матрицы $(W^Q, W^K, Q^V)$
2) перемножая эмбеддинг на каждую матрицу мы получаем вектора $(Q, K, V)$

- [Q]ueries - то, что сравниваем 
- [K]eys - то, с чем сравниваем 
- [V]alues -  результирующее значение которое мы умножаем на вес после этапа 5

3) Чтобы посчитать связь двух векторов (dot-product) мы должны умножить скалярно вектор $Q_1$ на вектор $K_2$ 
Чем больше значение скалярного произведения (dot-product) тем больше будет score связи слов. 
4) так же его стоит нормализировать $\sqrt{d_k}$ или поделить на 8 во избежание взрывов или затуханий градиентов. Стоит так делать на всех self-attentions чтобы полученные матрицы имели нормальные распределения. 
5) Полученные значения передаем в softmax чтобы все выходы в сумме имели значение 1: вектор равен значение поделить на сумму экспонент значений 
6) считаем $z_i$ путем перемножения полученных значений на их вектора $V_i$
7) на выходе получается квадратная матрица 

> $d_k$​ — размерность пространства ключей (и запросов, поскольку обычно они одинаковы). Скалярное произведение векторов масштабается пропорционально размерности векторов. То есть, если размерность векторов $d_k$​ велика, значения скалярных произведений $Q\times K^T$ также будут большими. Это может приводить к численным нестабильностям и неадекватным распределениям значений после применения softmax функции.

Виды Attention: 
- Full $n^2$ attention
	- Расчет для всех имеющихся слов 
- Sliding window attention
	- Расчет на n шагов слева и справа от текущего
- Global+sliding window

![](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6dc78ea6-337d-4d39-8c13-f733bf792b37_810x1080.gif)



### MultiHead Self-Attention 
Head - голова - триплет матриц QKV

Множественный слой самовнимания отличается от обычного тем что мы будем собирать разные триплеты матриц Q, K и V. Например связь слова подлежащего с местоимениями, или связь слова с предыдущим словом, и тд.
Формула векторного вычисления выходного эмбеддинга
$$\LARGE\text{softmax}(\frac{Q \times K^T}{\sqrt{d_k}})V = Z$$
Весь этап Attention слоя 
![[Pasted image 20240722002221.png]]

---

## Позиционное кодирование 

Вместе с матрицей эмбеддингов токенов (представь ключи под которыми лежат токены после токенизации) мы создаем матрицу той же размерности что и матрица эмбеддингов токинов, но под теми же индексами хранятся позиции слов из исходного предложения.

#### Функции для инициализации матрицы позиционного кодирования

$\Large 1. PE(pos, 2i) = \sin(\frac{pos}{10000^{\left(\frac{2i}{d_{\text{model}}}\right)}})$

$\Large 2. PE(pos, 2i+1) = \cos(\frac{pos}{10000^{\left(\frac{2i}{d_{\text{model}}}\right)}})$

но чем больше становится трансформер, тем проще использовать рандомную инициализацию, чтобы модель сама определяла что для нее важно на этапе обучения

----

### Residual layers 

Это дополнение к обучению, которое исходный эмбеддинг дополнительно проводит в обход self-attention блока суммируя "прокинутые" эмбеддинги с выходами из self-attention. Этот метод позволяет избежать затухания градиента для глубоких сетей. 

```python
def forward(self, x, sublayer):
	return x + self.dropout(sublayer(self.norm(x)))
```

### LayerNorm 

Слой нормализации. Его лучше использовать вместо BatchNorm

$$A * \frac{X -  mean(X)}{std(X) + b}$$

```python
class LayerNorm(nn.Module):
	'''
	считает по эмбеддингам среднее и стандартное отклонение
	делает стандартизацию
	'''
	def __init__(self, features, eps=1e-6):
		super(LayerNorm, self).__init__()
		# чтобы каждая из размерностей эмбеддинга масштабировалась (смещалась)
		self.a_2 = nn.Parameter(torch.ones(features)) # нормализующий терм 
		self.b_2 = nn.Parameter(torch.zeros(features)) # веса для смещения
		self.eps = eps
	
	def forward(self, x):
		mean = x.mean(-1, keepdim=True)
		std = x.std(-1, keepdim=True)
		return self.a_2 * (x - mean) / (std + self.eps) + self.b_2
```

### Post LayerNorm

Производим нормализацию после того как посчитали какой-то блок
$x_l \Rightarrow {\color{orange}\text{[Multi-Head Attention]}} \Rightarrow {\color{white}\text{[addition]}} \rightarrow {\color{green}\text{[Layer Norm]}} \Rightarrow {\color{aqua}\text{[FFN]}} \Rightarrow {\color{white}\text{[addition]}} \rightarrow {\color{green}\text{[Layer Norm]}} \rightarrow x_{l+1}$

#### Pre LayerNorm

Использование слоя нормализации перед подсчетом какого-либо блока
$x_l \Rightarrow {\color{green}\text{[Layer Norm]}} \rightarrow{\color{orange}\text{[Multi-Head Attention]}} \Rightarrow {\color{white}\text{[addition]}} \Rightarrow {\color{green}\text{[Layer Norm]}} \rightarrow {\color{aqua}\text{[FFN]}} \Rightarrow {\color{white}\text{[addition]}}  \rightarrow x_{l+1}$

---
---

# Decoder

## Encoder-Decoder Attention / Cross Attention 

Если задача машинного перевода, этот блок нужен для начала написания текста на другом языке 

---
---
---


